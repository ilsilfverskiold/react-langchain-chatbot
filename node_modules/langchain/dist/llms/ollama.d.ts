import { LLM, BaseLLMParams } from "./base.js";
import { OllamaInput, OllamaCallOptions } from "../util/ollama.js";
import { CallbackManagerForLLMRun } from "../callbacks/manager.js";
import { GenerationChunk } from "../schema/index.js";
export declare class Ollama extends LLM implements OllamaInput {
    CallOptions: OllamaCallOptions;
    static lc_name(): string;
    lc_serializable: boolean;
    model: string;
    baseUrl: string;
    mirostat?: number;
    mirostatEta?: number;
    mirostatTau?: number;
    numCtx?: number;
    numGpu?: number;
    numThread?: number;
    repeatLastN?: number;
    repeatPenalty?: number;
    temperature?: number;
    stop?: string[];
    tfsZ?: number;
    topK?: number;
    topP?: number;
    constructor(fields: OllamaInput & BaseLLMParams);
    _llmType(): string;
    invocationParams(options?: this["ParsedCallOptions"]): {
        model: string;
        options: {
            mirostat: number | undefined;
            mirostat_eta: number | undefined;
            mirostat_tau: number | undefined;
            num_ctx: number | undefined;
            num_gpu: number | undefined;
            num_thread: number | undefined;
            repeat_last_n: number | undefined;
            repeat_penalty: number | undefined;
            temperature: number | undefined;
            stop: string[] | undefined;
            tfs_z: number | undefined;
            top_k: number | undefined;
            top_p: number | undefined;
        };
    };
    _streamResponseChunks(input: string, options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;
    /** @ignore */
    _call(prompt: string, options: this["ParsedCallOptions"]): Promise<string>;
}
